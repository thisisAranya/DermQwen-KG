# -*- coding: utf-8 -*-
"""Qwen_KG_gradio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nZ4AbOTKstn9WlIGzx3A67two29Dwgv8

### Dependencies
"""

# Install essential libraries
!pip install -q transformers huggingface_hub networkx rdflib datasets
!pip install -q bitsandbytes
!pip install -q sentence-transformers
!pip install -q --upgrade transformers
!pip install -q "qwen-vl-utils[decord]==0.0.8"

"""### Datasets"""

# Step 1: Save your kaggle.json inside the notebook
import json
import os

# Create the .kaggle directory
os.makedirs('/root/.kaggle', exist_ok=True)

# Write your kaggle token
kaggle_token = {
    "username": "aranyasaha",
    "key": "e6edd39652ec8bf7a896420248e50803"
}

with open('/root/.kaggle/kaggle.json', 'w') as file:
    json.dump(kaggle_token, file)

# Set file permissions
os.chmod('/root/.kaggle/kaggle.json', 600)

print("✅ Kaggle API token is set up!")

# Step 2: Install kaggle library
!pip install -q kaggle

# Step 3: Authenticate Kaggle API
from kaggle.api.kaggle_api_extended import KaggleApi

api = KaggleApi()
api.authenticate()

print("✅ Kaggle API authenticated!")

# Step 4: Download and unzip datasets

# Download the first dataset
print("⬇️ Downloading zxzzzzzzzzzzzzzz...")
api.dataset_download_files('chapkhabo/zxzzzzzzzzzzzzzz', path='./', unzip=True)
print("✅ First dataset downloaded and unzipped!")

# Create a 'dataset' folder if it doesn't exist
if not os.path.exists('dataset'):
    os.mkdir('dataset')

# Download the second dataset inside 'dataset'
print("⬇️ Downloading selective-dermnet-for-llm into dataset/...")
api.dataset_download_files('aranyasaha/selective-dermnet-for-llm', path='./dataset', unzip=True)
print("✅ Second dataset downloaded and unzipped!")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

import kagglehub

# Download latest version
path = kagglehub.model_download("aranyasaha/dino-model-trained-on-dermnet/pyTorch/default")

print("Path to model files:", path)

"""### Import Libraries"""

import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from rdflib import Graph, Literal, RDF, RDFS
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from datasets import load_dataset
from huggingface_hub import hf_hub_download

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

"""### DINO Model as Auxiliary Classifier"""

dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')

class DinoVisionTransformerClassifier(nn.Module):
    def __init__(self):
        super(DinoVisionTransformerClassifier, self).__init__()
        self.transformer = dinov2_vits14
        self.classifier = nn.Sequential(
            nn.Linear(384, 256),
            nn.ReLU(),
            nn.Linear(256, 8)
        )

    def forward(self, x):
        x = self.transformer(x)
        x = self.transformer.norm(x)
        x = self.classifier(x)
        return x

model1 = DinoVisionTransformerClassifier()

model1.load_state_dict(torch.load('/kaggle/input/dino-model-trained-on-dermnet/pytorch/default/1/best_model.pth', weights_only=True))

model1.to('cuda')

"""### Vision Language Model"""

import torch
from transformers import (
    Qwen2_5_VLForConditionalGeneration,
    Qwen2_5_VLProcessor,
    BitsAndBytesConfig
)

# Model ID
model_id = "Aranya31/DermQwen-7b-adapter"
or_model_id = "Qwen/Qwen2.5-VL-7B-Instruct"

# BitsAndBytesConfig for int-4 quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load model with quantization and optimized memory usage
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    model_id,
    low_cpu_mem_usage=True,
    torch_dtype=torch.bfloat16,
    device_map={"": device},  # Ensure all layers are mapped to cuda:0
    quantization_config=bnb_config
).to(device)

# Load processor
processor = Qwen2_5_VLProcessor.from_pretrained(or_model_id)

model.to('cuda')

"""### Knowledge Graph Retriever"""

from sentence_transformers import SentenceTransformer, util
G_loaded = nx.read_graphml("/content/knowledge_graph.graphml")

# Load a retrieval model for RAG
retrieval_model = SentenceTransformer("all-MiniLM-L6-v2")

# Convert KG nodes to embeddings
entity_embeddings = {node: retrieval_model.encode(node) for node in G_loaded.nodes}

"""### Necessary Functions"""

import torch
from PIL import Image
from torchvision import transforms
import torch.nn.functional as F
import os
import csv
import re

# List of skin disease labels
disease_labels = [
    'actinic keratosis',
    'basal cell carcinoma',
    'dermatitis',
    'lichen planus',
    'melanoma',
    'psoriasis',
    'rosacea',
    'Seborrheic keratosis'
]

# Preprocessing function
def preprocess_image(image):
    preprocess = transforms.Compose([
        transforms.Resize((336, 336)),
        transforms.ToTensor(),
    ])
    # image = Image.open(image_path)
    return preprocess(image).unsqueeze(0)

# Function to perform inference and return the disease name and probability
def predict_skin_disease(model, image_path):
    image = Image.open(image_path)
    input_tensor = preprocess_image(image).to('cuda')

    model.eval()
    with torch.no_grad():
        output = model(input_tensor)

    probabilities = F.softmax(output, dim=1)
    predicted_index = torch.argmax(probabilities, dim=1).item()

    predicted_probability = probabilities[0, predicted_index].item()
    predicted_disease = disease_labels[predicted_index]

    return predicted_probability, predicted_disease

# Extract Assistant's response
def extract_response(text):
    match = re.search(r"ASSISTANT:\s*(.*)", text, re.DOTALL)
    return match.group(1).strip() if match else None

def extract_after_second_assistant(text):
    # Find all occurrences of "assistant"
    split_text = text.split('assistant')
    if len(split_text) > 2:
        # Join everything after the second "assistant"
        return 'assistant'.join(split_text[2:]).strip()
    else:
        return None

import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForCausalLM

# Define the conversation
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "What is this?"},
            {"type": "image"},
        ],
    },
]

# Apply chat template
prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

# Load the image
image_path = "/content/dataset/test_merged_selective_resized/test_merged_selective_resized/known_39/Psoriasis-Guttate-45.jpg"
raw_image = Image.open(image_path)

# Prepare inputs
device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = processor(images=raw_image, text=prompt, return_tensors="pt").to(device)

# Generate output
output = model.generate(**inputs, max_new_tokens=200, do_sample=False)

# Decode and print response
response = processor.tokenizer.decode(output[0], skip_special_tokens=True)
print(response)

import torch
from sentence_transformers import util
from PIL import Image
from transformers import AutoProcessor, AutoModelForCausalLM

def rag_query(query, retrieval_model, entity_embeddings, image_path, graph, threshold, model, processor, model1):
    # Encode the query
    query_embedding = retrieval_model.encode(query)

    # Find the best matching entity from the KG
    best_match = max(entity_embeddings.items(), key=lambda item: util.cos_sim(query_embedding, item[1]))
    best_entity = best_match[0]

    # Retrieve relations associated with the best entity
    related_entities = [(target, data['relation']) for target, data in graph[best_entity].items()]
    relation_text = " ".join([f"{best_entity} -({relation})-> {target}" for target, relation in related_entities])

    # Construct system prompt
    system_prompt = "You are a helpful AI assistant for medical information and recommendations."
    prompt_text = "What is the name of the disease?"

    # Load image
    raw_image = Image.open(image_path)

    # Ensure correct message format
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": prompt_text},
            ],
        }
    ]

    # Apply chat template
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

    # Prepare inputs
    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = processor(images=raw_image, text=prompt, return_tensors="pt").to(device)

    # Generate output
    output = model.generate(**inputs, max_new_tokens=64, do_sample=False)

    disease_name = processor.tokenizer.decode(output[0], skip_special_tokens=True)

    text_lower = disease_name.lower()

    # Match disease name
    matched_disease = None
    for disease in disease_labels:
        if disease.lower() in text_lower:
            matched_disease = disease
            break

    disease_label = matched_disease

    # Predict skin disease
    probability, predicted_disease = predict_skin_disease(model1, image_path)

    if probability > threshold:
        additional_context = f"The name of the disease is {predicted_disease}"
        source = "Auxiliary Classifier"
    else:
        additional_context = f'The name of the disease is {disease_label}'
        source = "Vision Language Model"

    # Construct detailed prompt
    prompt_text = (
        f"Using knowledge about {best_entity} and its relations ({relation_text}), "
        f"answer the question in detail: {additional_context} {query}"
    )

    # Ensure correct message format
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": prompt_text},
            ],
        }
    ]

    # Apply chat template
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

    # Prepare inputs
    inputs = processor(images=raw_image, text=prompt, return_tensors="pt").to(device)

    # Generate output
    output = model.generate(**inputs, max_new_tokens=1024, do_sample=False)
    detailed_response = processor.tokenizer.decode(output[0], skip_special_tokens=True)

    return (source), (additional_context), (detailed_response)

"""### Single Inference"""

import re

# Define image path and query
image_path = "/content/dataset/test_merged_selective_resized/test_merged_selective_resized/known_39/Psoriasis-Guttate-45.jpg"
query = "What is the treatment of the disease?"
softmax_threshold = 0.9

# Call RAG query function
source, disease_name, response = rag_query(
    query,
    retrieval_model,
    entity_embeddings,
    image_path,
    G_loaded,
    softmax_threshold,
    model,
    processor,
    model1
)

# Display output
print("Label Source:", source, "\n")
print("Disease Name:", disease_name, "\n")
print("User:", query, "\n")
print("Answer:", extract_after_second_assistant(response))

"""## Gradio"""

!pip install gradio

import gradio as gr
import re

# Gradio app logic
def run_rag_pipeline(image, query, softmax_threshold):
    # image will be a path (if type="filepath"), so no issue
    image_path = image

    # Call your RAG function
    source, disease_name, response = rag_query(
        query,
        retrieval_model,
        entity_embeddings,
        image_path,
        G_loaded,
        softmax_threshold,
        model,
        processor,
        model1
    )

    cleaned_answer = extract_after_second_assistant(response)

    return source, disease_name, query, cleaned_answer

# Build Gradio app
demo = gr.Interface(
    fn=run_rag_pipeline,
    inputs=[
        gr.Image(type="filepath", label="Upload Skin Image"),
        gr.Textbox(lines=2, placeholder="Enter your question here...", label="Query"),
        gr.Slider(0.0, 1.0, value=0.9, step=0.01, label="Softmax Threshold"),
    ],
    outputs=[
        gr.Textbox(label="Label Source"),
        gr.Textbox(label="Predicted Disease Name"),
        gr.Textbox(label="User Query"),
        gr.Textbox(label="Cleaned Assistant Response")
    ],
    title="Skin Disease Diagnosis with RAG",
    description="Upload an image and enter a question. The model will predict the disease and answer your query.",
)

demo.launch()